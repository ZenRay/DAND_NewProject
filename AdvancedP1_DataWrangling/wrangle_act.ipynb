{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pprint\n",
    "import time\n",
    "import copy\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data directory, remove the files if they exits\n",
    "folder_name = \"./data\"\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "else:\n",
    "    for i in os.scandir(folder_name):\n",
    "        try:\n",
    "            os.remove(i)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from url\n",
    "image_prediction_url = \"https://raw.githubusercontent.com/udacity/new-dand-advanced-china/master/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/WeRateDogs%E9%A1%B9%E7%9B%AE/image-predictions.tsv\"\n",
    "twitter_archive_enhanced_url = \"https://raw.githubusercontent.com/udacity/new-dand-advanced-china/master/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/WeRateDogs%E9%A1%B9%E7%9B%AE/twitter-archive-enhanced.csv\"\n",
    "tweet_json_url = \"https://raw.githubusercontent.com/udacity/new-dand-advanced-china/master/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/WeRateDogs%E9%A1%B9%E7%9B%AE/tweet_json.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data by using a function\n",
    "def get_data(url, path):\n",
    "    \"\"\"\n",
    "    get the data from the url. Return the path of the data\n",
    "    file\n",
    "    \n",
    "    Args:\n",
    "    (str) url - the data address on the web\n",
    "    (str) path - the parent path which stores the data\n",
    "    \n",
    "    Returns:\n",
    "    (str) result - the file path which store the data\n",
    "    \"\"\"\n",
    "    \n",
    "    result = os.path.join(path, os.path.basename(url))\n",
    "    \n",
    "    url_request = requests.get(url)\n",
    "    \n",
    "    with open(result, \"wb\") as file:\n",
    "        file.write(url_request.content)\n",
    "    # adjust the sleep time to increase requesting interval\n",
    "    time.sleep(3)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the datas by using the url and the function\n",
    "if True:\n",
    "    image_prediction = get_data(image_prediction_url, folder_name)\n",
    "    twitter_archive_enhanced = get_data(twitter_archive_enhanced_url, folder_name)\n",
    "    tweet_json = get_data(tweet_json_url, folder_name)\n",
    "else:\n",
    "    image_prediction = \"./data/image-predictions.tsv\"\n",
    "    twitter_archive_enhanced = \"./data/twitter-archive-enhanced.csv\"\n",
    "    tweet_json = \"./data/tweet_json.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the files, which does just check the files\n",
    "import filecmp\n",
    "if False:\n",
    "    print(filecmp.cmp(image_prediction, \"./Test/image-predictions.tsv\"), \n",
    "          filecmp.cmp(twitter_archive_enhanced, \"./Test/twitter-archive-enhanced.csv\"),\n",
    "          filecmp.cmp(tweet_json, \"./Test/tweet_json.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 收集数据\n",
    "1. 本次项目中数据已经由 `project` 给出了相关的下载链接，因此先直接使用了 `requests` 库获取相关的数据，保留了原始的文件名。另外因为是对数据获取是使用 `requests` 进行下载，通过完全将数据通过 `binary` 写入文件。为了验证得到数据的准确性，使用了 `filecmp` 对直接下载的数据和 `requests` 得到的数据进行比较。\n",
    "\n",
    "2. 本次项目中得到了三种类型文件： `csv`, `tsv` 以及 `txt`。前两种文件类型比较容易处理，通过 `pandas` 的相关读取方法即可得到数据内容；但是 `txt` 的文件比较有难度，体现在——a）文件内容其实是 `json` 类型的；b）通过 `open` 的方式读取得到的数据是一个字符串，需要对数据内容进行相关处理。c）对数据进行相关的提取，一方面是因为在实际情况中并非是所有的数据都是必须的。对某些数据需要合适转换才能完成数据完整读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datas\n",
    "twitter_archive_data = pd.read_csv(twitter_archive_enhanced)\n",
    "\n",
    "image_prediction_data = pd.read_csv(image_prediction, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.DataFrame()\n",
    "index_count = -1\n",
    "\n",
    "with open(tweet_json, \"rb\") as file:\n",
    "    for i in file.readlines():\n",
    "        temp = []\n",
    "        dict_data = json.loads(i)\n",
    "        temp.append({\"id\": dict_data[\"id\"], \n",
    "                     \"created_at\": dict_data[\"created_at\"], \n",
    "                     \"full_text\": dict_data[\"full_text\"], \n",
    "                     \"retweet_count\":dict_data[\"retweet_count\"], \n",
    "                     \"favorite_count\":dict_data[\"favorite_count\"],\n",
    "                     \"in_reply_to_user_id\": dict_data[\"in_reply_to_user_id\"],\n",
    "                     \"in_reply_to_status_id\": dict_data[\"in_reply_to_status_id\"],\n",
    "                     \"source\": dict_data[\"source\"]})\n",
    "        # maybe dict data can't contain the key\n",
    "        try:\n",
    "            temp[0][\"media_url\"] =  dict_data[\"entities\"][\"media\"][0][\"media_url_https\"]\n",
    "        except KeyError:\n",
    "            temp[0][\"media_url\"] =  None\n",
    "            \n",
    "        try:\n",
    "            temp[0][\"expanded_url\"] =  dict_data[\"extended_entities\"][\"media\"][0][\"expanded_url\"]\n",
    "        except KeyError:\n",
    "            temp[0][\"expanded_url\"] =  None\n",
    "            \n",
    "            \n",
    "        temp = pd.DataFrame(temp, columns=[\"id\", \"created_at\", \"full_text\", \"media_url\",\n",
    "                                           \"favorite_count\", \"retweet_count\", \"in_reply_to_status_id\",\n",
    "                                           \"in_reply_to_user_id\", \"expanded_url\", \"source\"],\n",
    "                            index=pd.Index([index_count+1]))\n",
    "        tweet_data = pd.concat([tweet_data, temp])\n",
    "        \n",
    "        # increase the index value\n",
    "        index_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the keys\n",
    "def print_fields(column_data, name):\n",
    "    \"\"\"\n",
    "    display the information about the data fields\n",
    "    \n",
    "    Args:\n",
    "    (Array) column_data - the fields about the data\n",
    "    (str) name - the name of the data variable\n",
    "    \n",
    "    Returns:\n",
    "    (None)\n",
    "    \"\"\"    \n",
    "    print(\"There are %d fields in the %s:\\n\" % (len(column_data), name))\n",
    "\n",
    "    for i, key in enumerate(column_data):\n",
    "        print(\"{0:<2}\\t{1:<30}\".format(i+1, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_prediction_data\n",
    "print_fields(image_prediction_data.columns, \"image_prediction_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_archive_data\n",
    "print_fields(twitter_archive_data.columns, \"twitter_archive_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_data\n",
    "print_fields(tweet_data.columns, \"tweet_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 评估数据\n",
    "#### 2.1 对数据的 `field` 进行评估\n",
    "对 `tsv` 和 `csv` 文件采取了直接使用 `pandas` 读取数据，直接得到了两者的 `field`：其中 `image-prediction.tsv` 中有 `12` 个 `fields`，得到的结果如下：\n",
    "\n",
    "1. tweet_id : 推特链接中位于 \"status/\" 后面的一部分\n",
    "2. jpg_url:预测的图像资源链接\n",
    "3. img_num : 最可信的预测结果对应的图像编号\n",
    "4. p1 : 是算法对推特中图片的一号预测\n",
    "5. p1_conf : 是算法预测的可信度\n",
    "6. p1_dog : 预测该图片是否属于“狗”\n",
    "7. p2 : 算法对推特中图片预测的第二种可能性\n",
    "8. p2_conf : 第二种算法的预测的可信度\n",
    "9. p2_dog : 第二种算法预测该图片是否属于“狗”\n",
    "10. p3 : 算法对推特中图片预测的第三种可能性\n",
    "11. p3_conf : 第三种算法的预测的可信度\n",
    "12. p3_dog : 第三种算法预测该图片是否属于“狗”\n",
    "\n",
    "\n",
    "而 `twitter-archive-enhanced.csv` 中有 `17` 个 `fields`，得到的结果如下：\n",
    "\n",
    "1. tweet_id : 推特链接中位于 \"status/\" 后面的最后一部分\n",
    "2. in_reply_to_status_id : 代表是否有回复，如果有保留了 `tweet_id` 的数据，以数字形式保存\n",
    "3. in_reply_to_user_id : 代表是否有回复，如果有保留了 `tweet_id` 的数据，以字符串形式保存\n",
    "4. timestamp : 创建时间\n",
    "5. source : 发送本条 `tweet` 时的设备信息，以 `HTML` 标签样式保存数据\n",
    "6. text : 发送的 `tweet` 信息，以 `utf-8` 的字符串形式保存数据\n",
    "7. retweeted_status_id : 第一个转 `tweet` 用户的信息\n",
    "8. retweeted_status_user_id : 第一个转 `tweet` 用户的信息\n",
    "9. retweeted_status_timestamp : 第一个转 `tweet` 用户的信息中的时间\n",
    "10. expanded_urls : `tweet` 的 `entities` 中 `url`\n",
    "11.\trating_numerator : 评分分数的分子\n",
    "12.\trating_denominator : 评分分数的分母\n",
    "13.\tname : 狗的品种名称\n",
    "14.\tdoggo : 狗是否属于该类型\n",
    "15.\tfloofer : 狗是否属于该类型\n",
    "16.\tpupper : 狗是否属于该类型\n",
    "17.\tpuppo : 狗是否属于该类型\n",
    "\n",
    "\n",
    "因为 `tweet_json.txt` 文件需要将数据转化为 `json` 类型，额外提取相关数据信息进行组合。本次提取数据是参考以上两个文件中的已有的 `field` 进行分析，一方面是想通过该数据验证以上数据是否正确，另一方面也是为了验证数据清洗的结果是否合理及正确。具体提取的 `field` 如下：\n",
    "\n",
    "1. id : 推特链接中位于 \"status/\" 后面的最后一部分，以数字形式保存\n",
    "2. created_at : `tweet` 发送时间\n",
    "3. full_text : 发送的 `tweet` 信息，以 `utf-8` 的字符串形式保存数据\n",
    "4. media_url : 狗的图像资源链接\n",
    "5. favorite_count : 表示被 `twitter` 用户喜欢的数量\n",
    "6. retweet_count : 表示被转发 `tweet` 的数量\n",
    "7. in_reply_to_status_id : 代表是否有回复，如果有保留了 `tweet_id` 的数据，以数字形式保存\n",
    "8. in_reply_to_user_id : 代表是否有回复，如果有保留了 `tweet_id` 的数据，以字符串形式保存\n",
    "9. expanded_url : \n",
    "10. source : 发送本条 `tweet` 时的设备信息，以 `HTML` 标签样式保存数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_data(df, name):\n",
    "    \"\"\"\n",
    "    detect the data information about missing value, data types\n",
    "    and unique values\n",
    "    \n",
    "    Args:\n",
    "    (dataframe) df - dataframe storing the data\n",
    "    (str) name - variable name referring  the data\n",
    "    \n",
    "    Returns:\n",
    "    (None)\n",
    "    \"\"\"\n",
    "    print(\"The summary information:\\n\")\n",
    "    print(\"There are %d fiels and %d data points in the %s.\\n\" % (df.shape[1], df.shape[0], name))\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"The missing_value information about the %s:\\n\" % name)\n",
    "    print(df.isnull().sum())\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"The unique value about every field:\\n\")\n",
    "    \n",
    "    for i in df.columns:\n",
    "        print(\"The %s field with %s dtype has unique values:\\n\" % (i, df[i].dtypes))\n",
    "        if len(df[i].unique()) > 5:\n",
    "            print(\",\\n\".join([str(df[i].unique()[element]) for element in range(5)]))\n",
    "        else:\n",
    "            print(\",\\n\".join([str(element) for element in df[i].unique()]))\n",
    "        print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "detect_data(image_prediction_data, \"image_prediction_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the missing value hist plot\n",
    "x_axis = image_prediction_data.columns.get_values()\n",
    "height = image_prediction_data.count() - image_prediction_data.isnull().sum()\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.bar(np.arange(len(x_axis)), height, tick_label=x_axis, facecolor=\"grey\", width=.8)\n",
    "loc, labels = plt.xticks(rotation=330, fontsize=13)\n",
    "plt.xlabel(\"Field\", fontsize=16)\n",
    "\n",
    "plt.hlines(y=2075, xmin=-0.5, xmax=loc.max()+1,colors=\"r\", linestyles=\"--\", linewidth=4,label=\"Total Data Point:2075\")\n",
    "plt.legend()\n",
    "plt.title(\"Fig Unmissing Value Count About Image Prediction\", loc=\"left\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "detect_data(twitter_archive_data, \"twitter_archive_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the missing value hist plot\n",
    "x_axis = twitter_archive_data.columns.get_values()\n",
    "height = len(twitter_archive_data) - twitter_archive_data.isnull().sum()\n",
    "hist_data = pd.DataFrame({\"field\": height.index.values, \"count\":height.values}, columns=[\"field\", \"count\"])\n",
    "\n",
    "hist_data.plot(x=\"field\", kind=\"bar\", figsize=(20, 17), rot=30, color=\"grey\") #, yticks=np.arange(height.min()-500, height.max()+200, 300))\n",
    "plt.xlabel(\"Field\", fontsize=16)\n",
    "loc, labels = plt.xticks( fontsize=10)\n",
    "plt.ylabel(\"Total Unmissing Value Count\", fontsize=15)\n",
    "plt.yticks(np.arange(0, height.max()+200, 100), fontsize=14)\n",
    "plt.title(\"Fig Unmissing Value Count About Twitter Archive Enhanced\", loc=\"left\", fontsize=20)\n",
    "\n",
    "\n",
    "for x, y, s in zip(loc, [i + 10 for i in height], [str(i) for i in height]):\n",
    "    plt.text(x-0.2, y, s, fontsize=12)\n",
    "    \n",
    "plt.hlines(y=2356, xmin=-0.5, xmax=loc.max()+1,colors=\"r\",\n",
    "           linestyles=\"--\", linewidth=4,label=\"Total Data Point:2356\")\n",
    "\n",
    "plt.legend(loc=(0.35, 0.8), fontsize=\"large\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "detect_data(tweet_data, \"tweet_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the missing value hist plot\n",
    "x_axis = tweet_data.columns.get_values()\n",
    "height = len(tweet_data) - tweet_data.isnull().sum()\n",
    "hist_data = pd.DataFrame({\"field\": height.index.values, \"count\":height.values}, columns=[\"field\", \"count\"])\n",
    "\n",
    "hist_data.plot(x=\"field\", kind=\"bar\", figsize=(20, 13), rot=30, color=\"grey\", legend=\"\") #, yticks=np.arange(height.min()-500, height.max()+200, 300))\n",
    "plt.xlabel(\"Field\", fontsize=16)\n",
    "loc, label = plt.xticks( fontsize=10)\n",
    "plt.ylabel(\"Total Unmissing Value Count\", fontsize=15)\n",
    "plt.yticks()\n",
    "plt.xticks(fontsize=15)\n",
    "\n",
    "for x, y, s in zip(loc, [i + 10 for i in height], [str(i) for i in height]):\n",
    "    plt.text(x-0.15, y, s, fontsize=15)\n",
    "\n",
    "plt.title(\"Fig Unmissing Value Count About Tweet Data\", loc=\"left\", fontsize=20)\n",
    "plt.hlines(y=2352, xmin=-0.5, xmax=loc.max()+1,colors=\"r\", linestyles=\"--\", linewidth=4,label=\"Total Data Point:2352\")\n",
    "plt.legend(loc=(0.59,0.8), fontsize=\"xx-large\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 评估数据\n",
    "#### 2.2 对数据内容及数据数据类型评估\n",
    "1. 对 `image_prediction_data` 进行数据内容和数据类型探索，该数据集有 `2075` 个数据且有 \t`12` 个 `field`；另外发存在数据类型不合适的情况，该数据集没有缺失值的情况。\n",
    "\n",
    "2. 对 `twitter_archive_data` 进行数据内容和数据类型探索，该数据集中有 `2356` 个数据且有 `17` 个 `field`；在数据类型不合适的情况以及该数据集某些 `field`  存在缺失值的情况—— `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp` 数据缺失严重；另外在 `expanded_urls` 中存在部分数据缺失\n",
    "\n",
    "3. 对 `tweet_data` 进行数据内容和数据类型探索，该数据集有 `10` 个 `field` 以及 `2352` 个数据点。该数据的主要问题，还是是数据类型不符合 `id`, `created_at`, `in_reply_to_status_id`, `in_reply_to_user_id`, `source`；一个行列单元格包含了多个信息， `full_text` 包括了 `tweet` 信息、评分分数以及链接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 总体评估结论：\n",
    "**质量**\n",
    "\n",
    "1. `image_prediction_data` 数据集\n",
    "\n",
    "\t* `tweet_id` 是 `int64` 的整数类型，实际应当保存为 `object` 类型\n",
    "\t* `p1` 数据中首字母存在大写和小写混用的情况\n",
    "\t* `p2` 数据中首字母存在大写和小写混用的情况\n",
    "\t* `p3` 数据中首字母存在大写和小写混用的情况\n",
    "\t* `p1_conf` 是比例数据，可以确认需要保留的小数点位数\n",
    "\t* `p2_conf` 是比例数据，可以确认需要保留的小数点位数\n",
    "\t* `p3_conf` 是比例数据，可以确认需要保留的小数点位数\n",
    "\n",
    "2. `twitter_archive_data`\t数据集\n",
    "\t\n",
    "\t* `tweet_id` 是 `int64` 的整数类型，实际应当保存为 `object` 类型\n",
    "\t* `in_reply_to_status_id` 是 `float64` 的整数类型，实际应当保存为 `object` 类型\n",
    "\t* `in_reply_to_user_id` 是 `float64` 的整数类型，实际应当保存为 `object` 类型\n",
    "\t* `timestamp` 是 `object` 的对象数据(字符串模式)，实际应当保存为 `datetime` 类型\n",
    "\t* `retweeted_status_id` 是 `float64` 的数值类型数据，实际应当是 `object` 类型\n",
    "\t* `retweeted_status_user_id` 是 `float64` 的数值类型数据，实际应当是 `object` 类型\n",
    "\t* `retweeted_status_timestamp` 是 `object` 的对象数据(字符串模式)，实际应当保存为 `datetime` 类型\n",
    "\t* 存在缺失值: 缺少相关信息，不合适进行处理\n",
    "\n",
    "3. `tweet_data`\t的数据集\n",
    "\n",
    "\t* `id` 是 `int64` 的整数类型，实际应当保存为 `object` 类型\n",
    "\t* `created_at` 是 `object` 的对象数据(字符串模式)，实际应当保存为 `datetime` 类型\n",
    "\n",
    "\n",
    "\n",
    "**整洁度**\n",
    "\n",
    "1. `image_prediction_data` 数据集\n",
    "\n",
    "\t目前尚未发现需要调整的结构问题\n",
    "\t\n",
    "2. `twitter_archive_data`\t数据集\n",
    "\n",
    "\t* `source` 中保存了 `HTML` 标签类型数据\n",
    "\t* `text` 中保存了评分数据，`tweet` 文本信息以及链接数据\n",
    "\t* `expanded_urls` 存储了重复数据值\n",
    "\t* `field` 拆分为了多个，例如`doggo`,`floofer`, `pupper`, `puppo`\n",
    "\n",
    "3. `tweet_data`\t的数据集\n",
    "\n",
    "\t* `full_text` 中保存了评分数据，`tweet` 文本信息以及链接数据\n",
    "\t* `source` 中保存了 `HTML` 标签类型数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data copy\n",
    "image_data_copy = copy.deepcopy(image_prediction_data)\n",
    "twitter_archive_data_copy = copy.deepcopy(twitter_archive_data)\n",
    "tweet_data_copy = copy.deepcopy(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before wrangling data about the image data:\\n\")\n",
    "print(image_data_copy.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `tweet_id` 是 `int64` 的整数类型，实际应当保存为 `object` 类型\n",
    "image_data_copy[\"tweet_id\"] = image_data_copy[\"tweet_id\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `p1` 数据中首字母存在大写和小写混用的情况\n",
    "# `p2` 数据中首字母存在大写和小写混用的情况\n",
    "# `p3` 数据中首字母存在大写和小写混用的情况\n",
    "image_data_copy[[\"p1\", \"p2\", \"p3\"]] = image_data_copy[[\"p1\", \"p2\", \"p3\"]].applymap(lambda x: x.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `p1_conf` 是比例数据，可以确认需要保留的小数点位数\n",
    "# `p2_conf` 是比例数据，可以确认需要保留的小数点位数\n",
    "# `p3_conf` 是比例数据，可以确认需要保留的小数点位数\n",
    "image_data_copy[[\"p1_conf\", \"p2_conf\", \"p3_conf\"]] = image_data_copy[[\"p1_conf\", \"p2_conf\", \"p3_conf\"]].applymap(lambda x: round(x, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After wrangling data about the image data:\\n\")\n",
    "print(image_data_copy.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before wrangling data about the twitter archive data:\\n\")\n",
    "print(twitter_archive_data_copy.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `tweet_id` 是 `int64` 的整数类型，实际应当保存为 `object` 类型\n",
    "twitter_archive_data_copy[\"tweet_id\"] = twitter_archive_data_copy[\"tweet_id\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `in_reply_to_status_id` 是 `float64` 的整数类型，实际应当保存为 `object` 类型\n",
    "# `in_reply_to_user_id` 是 `float64` 的整数类型，实际应当保存为 `object` 类型\n",
    "# `retweeted_status_id` 是 `float64` 的数值类型数据，实际应当是 `object` 类型\n",
    "# `retweeted_status_user_id` 是 `float64` 的数值类型数据，实际应当是 `object` 类型\n",
    "twitter_archive_data_copy[[\"in_reply_to_status_id\", \"in_reply_to_user_id\", \"retweeted_status_id\", \"retweeted_status_user_id\"]] = \\\n",
    "    twitter_archive_data_copy[[\"in_reply_to_status_id\", \"in_reply_to_user_id\", \"retweeted_status_id\", \"retweeted_status_user_id\"]].applymap(\n",
    "    lambda x: str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `timestamp` 是 `object` 的对象数据(字符串模式)，实际应当保存为 `datetime` 类型\n",
    "# `retweeted_status_timestamp` 是 `object` 的对象数据(字符串模式)，实际应当保存为 `datetime` 类型\n",
    "twitter_archive_data_copy[\"timestamp\"] = twitter_archive_data_copy[\"timestamp\"].apply(pd.Timestamp)\n",
    "twitter_archive_data_copy[\"retweeted_status_timestamp\"] = twitter_archive_data_copy[\"retweeted_status_timestamp\"].apply(pd.Timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_content(tag):\n",
    "    \"\"\"\n",
    "    get the data from the HTML tag. Return the content of the tag\n",
    "    \n",
    "    Args:\n",
    "    (str) tag - the HTML tag\n",
    "    \n",
    "    Returns:\n",
    "    (str) result - the content of the HTML tag\n",
    "    \"\"\" \n",
    "    soup = BeautifulSoup(tag, \"lxml\")\n",
    "    \n",
    "    result = soup.get_text()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `source` 中保存了 `HTML` 标签类型数据\n",
    "twitter_archive_data_copy[\"source\"] = twitter_archive_data_copy[\"source\"].apply(html_content)\n",
    "\n",
    "source_dict = {\"Twitter for iPhone\":\"iphone\", \n",
    "               \"Twitter Web Client\": \"web\", \n",
    "               \"Vine - Make a Scene\": \"vine\",\n",
    "               \"TweetDeck\": \"deck\"}\n",
    "\n",
    "twitter_archive_data_copy[\"source\"] = twitter_archive_data_copy[\"source\"].map(source_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(text, tweet_text_option=False, rating_numerator_option=False, rating_denominator_option=False):\n",
    "    \"\"\"\n",
    "    get the rate score from the text. Return the content of the tag\n",
    "    \n",
    "    Args:\n",
    "    (str) text - the tweet text\n",
    "    \n",
    "    Returns:\n",
    "    (tuple) result - the tuple contains the tweet text without the \n",
    "    rate score, rate scores\n",
    "    \"\"\" \n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    pattern = re.compile(\"\\s{0,1}(\\d{1,})/(\\d{0,3}).{0,}|\\s{.*\\w\\s{0,1}(https://.*\\w){0,1}\")\n",
    "\n",
    "    match_pattern = re.search(pattern, text)\n",
    "    \n",
    "    # regex the text\n",
    "    if match_pattern:\n",
    "        rating_numerator = int(match_pattern.group(1))\n",
    "        rating_denominator = int(match_pattern.group(2))\n",
    "\n",
    "        tweet_text = re.sub(pattern, \"\", text).strip()\n",
    "    else:\n",
    "        rating_numerator = np.nan\n",
    "        rating_denominator = np.nan\n",
    "        tweet_text = text\n",
    "    \n",
    "    # return the result\n",
    "    if tweet_text_option:\n",
    "        return tweet_text\n",
    "\n",
    "    if rating_denominator_option:\n",
    "        return rating_denominator\n",
    "\n",
    "    if rating_numerator_option:\n",
    "        return rating_numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `text` 中保存了评分数据，`tweet` 文本信息以及链接数据\n",
    "twitter_archive_data_copy[\"rating_denominator_1\"] = twitter_archive_data_copy[\"text\"].apply(lambda x: text_split(x, rating_denominator_option=True))\n",
    "twitter_archive_data_copy[\"rating_numerator_1\"] = twitter_archive_data_copy[\"text\"].apply(lambda x: text_split(x, rating_numerator_option=True))\n",
    "twitter_archive_data_copy[\"text\"] = twitter_archive_data_copy[\"text\"].apply(lambda x: text_split(x, tweet_text_option=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证提取到的 rating score 数据\n",
    "print(\"The non_difference between the archive data and the extract data about the rating score:\\n\")\n",
    "print(\"The non_difference about the rating denominator is %d\" % \\\n",
    "      sum(twitter_archive_data_copy[\"rating_denominator\"] == twitter_archive_data_copy[\"rating_denominator_1\"]))\n",
    "print(\"The non_difference about the rating numerator is %d\" % \\\n",
    "      sum(twitter_archive_data_copy[\"rating_numerator\"] == twitter_archive_data_copy[\"rating_numerator_1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `expanded_urls` 存储了重复数据值\n",
    "url = []\n",
    "for head, value, tail in zip(np.repeat(\"https://twitter.com/dog_rates/status/\", len(twitter_archive_data_copy)), \n",
    "                            twitter_archive_data_copy[\"tweet_id\"], np.repeat(\"/photo/1\", len(twitter_archive_data_copy))):\n",
    "    url.append(head+str(value)+tail)\n",
    "\n",
    "twitter_archive_data_copy[\"expanded_urls\"] = pd.Series(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  `field` 拆分为了多个，例如`doggo`,`floofer`, `pupper`, `puppo`\n",
    "dog_type = []\n",
    "for doggo, floofer, pupper, puppo in zip(twitter_archive_data_copy[\"doggo\"], \n",
    "                                         twitter_archive_data_copy[\"floofer\"],\n",
    "                                         twitter_archive_data_copy[\"pupper\"],\n",
    "                                         twitter_archive_data_copy[\"puppo\"]):\n",
    "    dog_type.append(doggo +  floofer + pupper + puppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_data_copy[\"dog_type\"] = pd.Series(dog_type)\n",
    "\n",
    "twitter_archive_data_copy[\"dog_type\"] = \\\n",
    "    twitter_archive_data_copy[\"dog_type\"].apply(lambda x: x.replace(\"None\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_data_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the duplicate data\n",
    "twitter_archive_data_copy.drop([\"doggo\", \"floofer\", \"pupper\", \"puppo\", \"rating_denominator_1\", \"rating_numerator_1\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After wrangling data about the twitter archive data:\\n\")\n",
    "print(twitter_archive_data_copy.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before wrangling data about the tweet data:\\n\")\n",
    "print(tweet_data_copy.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `id` 是 `int64` 的整数类型，实际应当保存为 `object` 类型\n",
    "tweet_data_copy[\"id\"] = tweet_data_copy[\"id\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  `created_at` 是 `object` 的对象数据(字符串模式)，实际应当保存为 `datetime` 类型\n",
    "tweet_data_copy[\"created_at\"] = tweet_data_copy[\"created_at\"].apply(pd.Timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `full_text` 中保存了评分数据，`tweet` 文本信息以及链接数据\n",
    "tweet_data_copy[\"rating_denominator_1\"] = tweet_data_copy[\"full_text\"].apply(lambda x: int(text_split(x, rating_denominator_option=True)))\n",
    "tweet_data_copy[\"rating_numerator_1\"] = tweet_data_copy[\"full_text\"].apply(lambda x: int(text_split(x, rating_numerator_option=True)))\n",
    "tweet_data_copy[\"full_text\"] = tweet_data_copy[\"full_text\"].apply(lambda x: text_split(x, tweet_text_option=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `source` 中保存了 `HTML` 标签类型数据\n",
    "tweet_data_copy[\"source\"] = tweet_data_copy[\"source\"].apply(html_content)\n",
    "\n",
    "tweet_data_copy[\"source\"] = tweet_data_copy[\"source\"].map(source_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After wrangling data about the tweet data:\\n\")\n",
    "print(tweet_data_copy.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_copy[\"expanded_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据清洗后流程\n",
    "以上经过数据清洗的过程，详情见 [wrangle_report](./wrangle_report.md)。完成以上的数据清洗，需要将相关数据进行融合：主要从几个方面去考虑，确立 **主键** ——因为需要依据主键值 `field` 来完成数据融合，另外还需要对各个 `field` 进行调整。在确立主键将数据融合到一起之后，需要再次对数据进行检查确认。至此才能保存数据结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_data_copy.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考\n",
    "1. [Tweet object — Twitter Developers](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html) 该文档对 `API` 数据的 `Field` 进行了解释，需要注意和目前得到的数据存在部分差异"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
